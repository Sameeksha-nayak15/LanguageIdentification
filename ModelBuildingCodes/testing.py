"""
Comprehensive Testing Script for Language Identifier (Joblib Version)
Tests multilingual texts, code-mixed inputs, and generates evaluation metrics
Works with pipeline .joblib file
"""

import pandas as pd
import numpy as np
from sklearn.metrics import (
    accuracy_score, 
    confusion_matrix, 
    classification_report,
    f1_score,
    precision_score,
    recall_score
)
import matplotlib.pyplot as plt
import seaborn as sns
from typing import List, Dict, Tuple, Optional
import joblib
import json
from collections import Counter

class LanguageIdentifierTester:
    def __init__(self, pipeline_path='saved_models/language_pipeline.joblib'):
        """
        Initialize tester with trained pipeline from joblib
        
        Args:
            pipeline_path: Path to .joblib file containing the complete pipeline
        """
        print(f"Loading pipeline from {pipeline_path}...")
        self.pipeline = joblib.load(pipeline_path)
        print(f"âœ“ Pipeline loaded successfully")
        
        # Extract components from pipeline
        if hasattr(self.pipeline, 'named_steps'):
            # It's a sklearn Pipeline
            print("âœ“ Detected sklearn Pipeline")
            self.vectorizer = self.pipeline.named_steps.get('vectorizer') or self.pipeline.named_steps.get('tfidf')
            self.model = self.pipeline.named_steps.get('classifier') or self.pipeline.named_steps.get('model')
            
            if self.vectorizer:
                print(f"âœ“ Extracted vectorizer: {type(self.vectorizer).__name__}")
            if self.model:
                print(f"âœ“ Extracted model: {type(self.model).__name__}")
        else:
            # Direct pipeline object
            self.vectorizer = None
            self.model = None
            print("âœ“ Using pipeline directly for predictions")
        
        # Get language classes
        self.languages = None
        if self.model and hasattr(self.model, 'classes_'):
            self.languages = self.model.classes_
        elif hasattr(self.pipeline, 'classes_'):
            self.languages = self.pipeline.classes_
        
        if self.languages is not None:
            print(f"âœ“ Found {len(self.languages)} languages: {', '.join(map(str, self.languages[:10]))}{'...' if len(self.languages) > 10 else ''}")
        else:
            print("âš  Warning: Could not determine language classes from pipeline")
        
        self.results = {}
    
    def predict(self, texts: List[str]) -> List[str]:
        """Predict languages for a list of texts"""
        if isinstance(texts, str):
            texts = [texts]
        
        try:
            # Use pipeline directly for prediction
            predictions = self.pipeline.predict(texts)
            return list(predictions)
            
        except Exception as e:
            print(f"\nError during prediction: {e}")
            print(f"Pipeline type: {type(self.pipeline)}")
            raise
    
    def predict_with_confidence(self, texts: List[str]) -> List[Tuple[str, float]]:
        """Predict languages with confidence scores"""
        if isinstance(texts, str):
            texts = [texts]
        
        try:
            # Predict using pipeline
            predictions = self.pipeline.predict(texts)
            
            # Get probabilities/confidence scores
            if hasattr(self.pipeline, 'predict_proba'):
                # Pipeline has predict_proba (e.g., with probability-based classifiers)
                probabilities = self.pipeline.predict_proba(texts)
            elif hasattr(self.pipeline, 'decision_function'):
                # Pipeline has decision_function (e.g., SVM)
                decision = self.pipeline.decision_function(texts)
                # Convert decision scores to confidence scores
                if len(decision.shape) == 1:
                    # Binary classification
                    probabilities = np.vstack([-decision, decision]).T
                else:
                    # Multi-class
                    probabilities = decision
                # Normalize to [0, 1] range (relative confidence)
                prob_min = probabilities.min(axis=1, keepdims=True)
                prob_max = probabilities.max(axis=1, keepdims=True)
                probabilities = (probabilities - prob_min) / (prob_max - prob_min + 1e-10)
            else:
                # No confidence scores available
                probabilities = np.ones((len(texts), 1))
            
            # Extract confidence scores
            results = []
            for i, pred in enumerate(predictions):
                if probabilities.shape[1] > 1:
                    # Get max confidence
                    confidence = np.max(probabilities[i])
                else:
                    confidence = 1.0
                results.append((pred, confidence))
            
            return results
            
        except Exception as e:
            print(f"\nError during prediction with confidence: {e}")
            raise
    
    def test_multilingual_texts(self) -> Dict:
        """Test with various multilingual texts"""
        print("\n" + "="*80)
        print("MULTILINGUAL TEXT TESTS")
        print("="*80)
        
        # Comprehensive multilingual test set
        test_data = {
            'eng': [
                "The quick brown fox jumps over the lazy dog.",
                "Machine learning is transforming the world.",
                "Python is a popular programming language."
            ],
            'spa': [
                "El rÃ¡pido zorro marrÃ³n salta sobre el perro perezoso.",
                "El aprendizaje automÃ¡tico estÃ¡ transformando el mundo.",
                "Python es un lenguaje de programaciÃ³n popular."
            ],
            'fra': [
                "Le rapide renard brun saute par-dessus le chien paresseux.",
                "L'apprentissage automatique transforme le monde.",
                "Python est un langage de programmation populaire."
            ],
            'deu': [
                "Der schnelle braune Fuchs springt Ã¼ber den faulen Hund.",
                "Maschinelles Lernen verÃ¤ndert die Welt.",
                "Python ist eine beliebte Programmiersprache."
            ],
            'ita': [
                "La veloce volpe marrone salta sopra il cane pigro.",
                "L'apprendimento automatico sta trasformando il mondo.",
                "Python Ã¨ un linguaggio di programmazione popolare."
            ],
            'por': [
                "A rÃ¡pida raposa marrom pula sobre o cachorro preguiÃ§oso.",
                "O aprendizado de mÃ¡quina estÃ¡ transformando o mundo.",
                "Python Ã© uma linguagem de programaÃ§Ã£o popular."
            ],
            'rus': [
                "Ð‘Ñ‹ÑÑ‚Ñ€Ð°Ñ ÐºÐ¾Ñ€Ð¸Ñ‡Ð½ÐµÐ²Ð°Ñ Ð»Ð¸ÑÐ° Ð¿Ñ€Ñ‹Ð³Ð°ÐµÑ‚ Ñ‡ÐµÑ€ÐµÐ· Ð»ÐµÐ½Ð¸Ð²ÑƒÑŽ ÑÐ¾Ð±Ð°ÐºÑƒ.",
                "ÐœÐ°ÑˆÐ¸Ð½Ð½Ð¾Ðµ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð¼ÐµÐ½ÑÐµÑ‚ Ð¼Ð¸Ñ€.",
                "Python - Ð¿Ð¾Ð¿ÑƒÐ»ÑÑ€Ð½Ñ‹Ð¹ ÑÐ·Ñ‹Ðº Ð¿Ñ€Ð¾Ð³Ñ€Ð°Ð¼Ð¼Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ."
            ],
            'zho': [
                "æ•æ·çš„æ£•è‰²ç‹ç‹¸è·³è¿‡æ‡’ç‹—ã€‚",
                "æœºå™¨å­¦ä¹ æ­£åœ¨æ”¹å˜ä¸–ç•Œã€‚",
                "Pythonæ˜¯ä¸€ç§æµè¡Œçš„ç¼–ç¨‹è¯­è¨€ã€‚"
            ],
            'jpn': [
                "ç´ æ—©ã„èŒ¶è‰²ã®ã‚­ãƒ„ãƒãŒæ€ ã‘è€…ã®çŠ¬ã‚’é£›ã³è¶Šãˆã‚‹ã€‚",
                "æ©Ÿæ¢°å­¦ç¿’ã¯ä¸–ç•Œã‚’å¤‰ãˆã¦ã„ã¾ã™ã€‚",
                "Pythonã¯äººæ°—ã®ã‚ã‚‹ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°è¨€èªžã§ã™ã€‚"
            ],
            'ara': [
                "Ø§Ù„Ø«Ø¹Ù„Ø¨ Ø§Ù„Ø¨Ù†ÙŠ Ø§Ù„Ø³Ø±ÙŠØ¹ ÙŠÙ‚ÙØ² ÙÙˆÙ‚ Ø§Ù„ÙƒÙ„Ø¨ Ø§Ù„ÙƒØ³ÙˆÙ„.",
                "Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ ÙŠØºÙŠØ± Ø§Ù„Ø¹Ø§Ù„Ù….",
                "Ø¨Ø§ÙŠØ«ÙˆÙ† Ù‡ÙŠ Ù„ØºØ© Ø¨Ø±Ù…Ø¬Ø© Ø´Ø¹Ø¨ÙŠØ©."
            ],
            'hin': [
                "à¤¤à¥‡à¤œ à¤­à¥‚à¤°à¥€ à¤²à¥‹à¤®à¤¡à¤¼à¥€ à¤†à¤²à¤¸à¥€ à¤•à¥à¤¤à¥à¤¤à¥‡ à¤•à¥‡ à¤Šà¤ªà¤° à¤•à¥‚à¤¦à¤¤à¥€ à¤¹à¥ˆà¥¤",
                "à¤®à¤¶à¥€à¤¨ à¤²à¤°à¥à¤¨à¤¿à¤‚à¤— à¤¦à¥à¤¨à¤¿à¤¯à¤¾ à¤•à¥‹ à¤¬à¤¦à¤² à¤°à¤¹à¤¾ à¤¹à¥ˆà¥¤",
                "à¤ªà¤¾à¤¯à¤¥à¤¨ à¤à¤• à¤²à¥‹à¤•à¤ªà¥à¤°à¤¿à¤¯ à¤ªà¥à¤°à¥‹à¤—à¥à¤°à¤¾à¤®à¤¿à¤‚à¤— à¤­à¤¾à¤·à¤¾ à¤¹à¥ˆà¥¤"
            ],
            'kor': [
                "ë¹ ë¥¸ ê°ˆìƒ‰ ì—¬ìš°ê°€ ê²Œìœ¼ë¥¸ ê°œë¥¼ ë›°ì–´ë„˜ìŠµë‹ˆë‹¤.",
                "ë¨¸ì‹  ëŸ¬ë‹ì´ ì„¸ìƒì„ ë³€í™”ì‹œí‚¤ê³  ìžˆìŠµë‹ˆë‹¤.",
                "íŒŒì´ì¬ì€ ì¸ê¸°ìžˆëŠ” í”„ë¡œê·¸ëž˜ë° ì–¸ì–´ìž…ë‹ˆë‹¤."
            ]
        }
        
        all_texts = []
        all_labels = []
        
        for lang, texts in test_data.items():
            all_texts.extend(texts)
            all_labels.extend([lang] * len(texts))
        
        # Predict
        predictions = self.predict(all_texts)
        
        # Calculate metrics
        accuracy = accuracy_score(all_labels, predictions)
        
        # Create results DataFrame
        results_df = pd.DataFrame({
            'text': all_texts,
            'true_label': all_labels,
            'predicted_label': predictions,
            'correct': [t == p for t, p in zip(all_labels, predictions)]
        })
        
        print(f"\nAccuracy: {accuracy*100:.2f}%")
        print(f"Correct: {results_df['correct'].sum()}/{len(results_df)}")
        
        # Language-wise accuracy
        print("\nPer-Language Results:")
        for lang in test_data.keys():
            lang_df = results_df[results_df['true_label'] == lang]
            if len(lang_df) > 0:
                lang_acc = lang_df['correct'].mean() * 100
                print(f"  {lang}: {lang_acc:.2f}% ({lang_df['correct'].sum()}/{len(lang_df)})")
        
        self.results['multilingual'] = {
            'accuracy': accuracy,
            'results_df': results_df,
            'test_data': test_data
        }
        
        return results_df
    
    def test_code_mixed_inputs(self) -> pd.DataFrame:
        """Test with code-mixed/bilingual texts"""
        print("\n" + "="*80)
        print("CODE-MIXED INPUT TESTS")
        print("="*80)
        
        code_mixed_texts = [
            # English-Spanish
            ("I love programming, pero tambiÃ©n me gusta el espaÃ±ol.", "eng_spa_mixed"),
            ("Let's go to the playa tomorrow.", "eng_spa_mixed"),
            
            # English-French
            ("Hello, comment allez-vous today?", "eng_fra_mixed"),
            ("C'est very interesting, n'est-ce pas?", "eng_fra_mixed"),
            
            # English-German
            ("Ich bin learning Python programming.", "eng_deu_mixed"),
            ("Das ist really cool!", "eng_deu_mixed"),
            
            # English-Japanese
            ("Hello world, ã“ã‚“ã«ã¡ã¯ä¸–ç•Œ", "eng_jpn_mixed"),
            ("I love sushi ã¨ã¦ã‚‚ç¾Žå‘³ã—ã„", "eng_jpn_mixed"),
            
            # English-Hindi
            ("à¤®à¥ˆà¤‚ Python à¤¸à¥€à¤– à¤°à¤¹à¤¾ à¤¹à¥‚à¤‚ and enjoying it.", "eng_hin_mixed"),
            ("This is à¤¬à¤¹à¥à¤¤ à¤…à¤šà¥à¤›à¤¾!", "eng_hin_mixed"),
            
            # Spanish-French
            ("Hola, comment Ã§a va?", "spa_fra_mixed"),
            
            # Multi-language
            ("Hello à¤¨à¤®à¤¸à¥à¤¤à¥‡ ä½ å¥½ Bonjour", "multi_mixed"),
        ]
        
        texts = [t[0] for t in code_mixed_texts]
        expected_types = [t[1] for t in code_mixed_texts]
        
        # Predict with confidence
        predictions = self.predict_with_confidence(texts)
        
        results_df = pd.DataFrame({
            'text': texts,
            'expected_type': expected_types,
            'predicted_language': [p[0] for p in predictions],
            'confidence': [p[1] for p in predictions]
        })
        
        print("\nCode-Mixed Text Results:")
        print(results_df.to_string(index=False))
        
        avg_confidence = results_df['confidence'].mean()
        print(f"\nAverage Confidence: {avg_confidence*100:.2f}%")
        print("\nNote: Lower confidence is expected for code-mixed texts.")
        
        self.results['code_mixed'] = results_df
        
        return results_df
    
    def test_edge_cases(self) -> pd.DataFrame:
        """Test edge cases"""
        print("\n" + "="*80)
        print("EDGE CASE TESTS")
        print("="*80)
        
        edge_cases = [
            ("Hi", "very_short"),
            ("a", "single_char"),
            ("123456789", "numbers_only"),
            ("!@#$%^&*()", "punctuation_only"),
            ("Hello Ð¼Ð¸Ñ€ ä¸–ç•Œ", "multi_script"),
            ("https://www.example.com", "url"),
            ("test@example.com", "email"),
            ("ðŸ˜€ðŸ˜ðŸ˜‚ðŸ¤£", "emoji_only"),
            ("       ", "whitespace_only"),
            ("The " * 100, "repetitive"),
        ]
        
        texts = [e[0] for e in edge_cases]
        case_types = [e[1] for e in edge_cases]
        
        predictions = self.predict_with_confidence(texts)
        
        results_df = pd.DataFrame({
            'case_type': case_types,
            'text': [t[:50] + ('...' if len(t) > 50 else '') for t in texts],
            'predicted_language': [p[0] for p in predictions],
            'confidence': [p[1] for p in predictions]
        })
        
        print("\nEdge Case Results:")
        print(results_df.to_string(index=False))
        
        self.results['edge_cases'] = results_df
        
        return results_df
    
    def evaluate_on_test_set(self, X_test, y_test) -> Dict:
        """Complete evaluation on test set"""
        print("\n" + "="*80)
        print("TEST SET EVALUATION")
        print("="*80)
        
        # Predict
        predictions = self.predict(X_test)
        
        # Calculate metrics
        accuracy = accuracy_score(y_test, predictions)
        f1_macro = f1_score(y_test, predictions, average='macro', zero_division=0)
        f1_weighted = f1_score(y_test, predictions, average='weighted', zero_division=0)
        precision = precision_score(y_test, predictions, average='weighted', zero_division=0)
        recall = recall_score(y_test, predictions, average='weighted', zero_division=0)
        
        print(f"\nOverall Metrics:")
        print(f"  Accuracy:            {accuracy*100:.2f}%")
        print(f"  F1-Score (Macro):    {f1_macro:.4f}")
        print(f"  F1-Score (Weighted): {f1_weighted:.4f}")
        print(f"  Precision:           {precision:.4f}")
        print(f"  Recall:              {recall:.4f}")
        
        # Confusion matrix
        cm = confusion_matrix(y_test, predictions)
        
        # Classification report
        report = classification_report(y_test, predictions, zero_division=0, output_dict=True)
        
        # Detailed results
        results_df = pd.DataFrame({
            'text': X_test,
            'true_label': y_test,
            'predicted_label': predictions,
            'correct': [t == p for t, p in zip(y_test, predictions)]
        })
        
        # Separate correct and incorrect
        correct_df = results_df[results_df['correct'] == True]
        incorrect_df = results_df[results_df['correct'] == False]
        
        print(f"\nCorrect Predictions: {len(correct_df)}")
        print(f"Incorrect Predictions: {len(incorrect_df)}")
        
        self.results['test_set'] = {
            'accuracy': accuracy,
            'f1_macro': f1_macro,
            'f1_weighted': f1_weighted,
            'precision': precision,
            'recall': recall,
            'confusion_matrix': cm,
            'classification_report': report,
            'results_df': results_df,
            'correct_df': correct_df,
            'incorrect_df': incorrect_df
        }
        
        return self.results['test_set']
    
    def create_confusion_matrix_plot(self, y_test, predictions, save_path='confusion_matrix.png'):
        """Create and save confusion matrix visualization"""
        # Get unique labels
        labels = sorted(list(set(y_test) | set(predictions)))
        
        # Calculate confusion matrix
        cm = confusion_matrix(y_test, predictions, labels=labels)
        
        # Plot
        plt.figure(figsize=(12, 10))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                    xticklabels=labels, yticklabels=labels,
                    cbar_kws={'label': 'Count'})
        plt.title('Confusion Matrix', fontsize=16, fontweight='bold')
        plt.xlabel('Predicted Language', fontsize=12)
        plt.ylabel('True Language', fontsize=12)
        plt.xticks(rotation=45, ha='right')
        plt.yticks(rotation=0)
        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"\nConfusion matrix saved to {save_path}")
        plt.close()
    
    def create_results_table(self, results_df: pd.DataFrame, save_path='results_table.csv'):
        """Create table of correctly identified vs misclassified languages"""
        print("\n" + "="*80)
        print("RESULTS TABLE: CORRECT vs MISCLASSIFIED")
        print("="*80)
        
        # Summary by language
        summary = []
        for lang in sorted(results_df['true_label'].unique()):
            lang_df = results_df[results_df['true_label'] == lang]
            correct = lang_df['correct'].sum()
            total = len(lang_df)
            incorrect = total - correct
            accuracy = (correct / total * 100) if total > 0 else 0
            
            summary.append({
                'Language': lang,
                'Total_Samples': total,
                'Correctly_Identified': correct,
                'Misclassified': incorrect,
                'Accuracy_%': accuracy
            })
        
        summary_df = pd.DataFrame(summary)
        summary_df = summary_df.sort_values('Accuracy_%', ascending=False)
        
        print("\nPer-Language Summary:")
        print(summary_df.to_string(index=False))
        
        # Save to CSV
        summary_df.to_csv(save_path, index=False)
        print(f"\nResults table saved to {save_path}")
        
        # Also save detailed results
        detailed_path = save_path.replace('.csv', '_detailed.csv')
        results_df.to_csv(detailed_path, index=False)
        print(f"Detailed results saved to {detailed_path}")
        
        return summary_df
    
    def generate_full_report(self, save_dir='evaluation_results'):
        """Generate comprehensive evaluation report"""
        import os
        os.makedirs(save_dir, exist_ok=True)
        
        print("\n" + "="*80)
        print("GENERATING COMPREHENSIVE REPORT")
        print("="*80)
        
        # Save all results
        report = {
            'summary': {
                'multilingual_accuracy': self.results.get('multilingual', {}).get('accuracy'),
                'test_set_accuracy': self.results.get('test_set', {}).get('accuracy'),
                'total_tests_run': len(self.results)
            }
        }
        
        # Save as JSON
        with open(f'{save_dir}/evaluation_report.json', 'w') as f:
            json.dump(report, f, indent=2)
        
        print(f"\nAll results saved to {save_dir}/")
        print("\nFiles generated:")
        print(f"  - evaluation_report.json")
        
        if 'multilingual' in self.results:
            print(f"  - Multilingual test results available")
        if 'code_mixed' in self.results:
            self.results['code_mixed'].to_csv(f'{save_dir}/code_mixed_results.csv', index=False)
            print(f"  - code_mixed_results.csv")
        if 'edge_cases' in self.results:
            self.results['edge_cases'].to_csv(f'{save_dir}/edge_case_results.csv', index=False)
            print(f"  - edge_case_results.csv")
        
        return report


def main():
    """Main testing pipeline"""
    print("="*80)
    print("LANGUAGE IDENTIFIER - COMPREHENSIVE TESTING SUITE (PIPELINE)")
    print("="*80)
    
    # Initialize tester with your pipeline .joblib file
    pipeline_path = 'saved_models/language_pipeline.joblib'  # Update this path
    
    tester = LanguageIdentifierTester(pipeline_path=pipeline_path)
    
    # Run all tests
    print("\n[1/4] Running Multilingual Tests...")
    multilingual_results = tester.test_multilingual_texts()
    
    print("\n[2/4] Running Code-Mixed Tests...")
    code_mixed_results = tester.test_code_mixed_inputs()
    
    print("\n[3/4] Running Edge Case Tests...")
    edge_case_results = tester.test_edge_cases()
    
    # If you have a test set, load and evaluate
    try:
        print("\n[4/4] Loading Test Set...")
        # Example: Load from your test data
        # test_df = pd.read_csv('test_data.csv')
        # X_test = test_df['text'].tolist()
        # y_test = test_df['language'].tolist()
        
        # test_results = tester.evaluate_on_test_set(X_test, y_test)
        # tester.create_confusion_matrix_plot(y_test, tester.predict(X_test))
        # tester.create_results_table(test_results['results_df'])
        
        print("\nTest set evaluation skipped (no test data loaded)")
        print("To use test set evaluation, uncomment and modify the code above")
    except Exception as e:
        print(f"\nTest set evaluation skipped: {e}")
    
    # Generate full report
    print("\n[5/5] Generating Comprehensive Report...")
    tester.generate_full_report()
    
    print("\n" + "="*80)
    print("TESTING COMPLETE!")
    print("="*80)
    print("\nTo test with your own text:")
    print(">>> text = 'Your text here'")
    print(">>> prediction = tester.predict([text])")
    print(">>> print(prediction)")


if __name__ == "__main__":
    main()